{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Pre-train a Detectron2 Backbone with Lightly"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we show how you can do self-supervised pre-training of a Detectron2 backbone with lightly. The focus of this tutorial is on how to get and store a pre-trained ResNet50 backbone of the popular Detectron2 framework."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many tasks in computer vision it can be beneficial to pre-train a neural network on a domain-specific dataset prior to finetuning it. For example, a retail detection network can be pre-trained with self-supervised learning on a large retail detection dataset. This way the neural network learns to extract relevant features from the images without requiring any annotations at all. As a consequence, it’s possible to finetune the network with only a handful of annotated images. This tutorial will guide you through the steps to pre-train a detection backbone from the popular Detectron2 framework."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this tutorial you will need to:\n",
    "\n",
    "* Install Lightly\n",
    "\n",
    "* Install Detectron2: Follow the instructions.\n",
    "\n",
    "* Download a dataset for pre-training (we will use the Freiburg Groceries Dataset dataset). You can download it by cloning the Github repository and running download_dataset.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you will need the Detectron2 configuration files. They are available here. In this tutorial we use a Faster RCNN with a feature pyramid network (FPN), so make sure you have the relevant file (Base-RCNN-FPN.yaml) in your directory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from detectron2 import config, modeling\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "\n",
    "from lightly.data import LightlyDataset\n",
    "from lightly.loss import NTXentLoss\n",
    "from lightly.models.modules import SimCLRProjectionHead\n",
    "from lightly.transforms import SimCLRTransform"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s set the configuration parameters for our experiments.\n",
    "\n",
    "We use a batch size of 512 and an input size of 128 in order to fit everything on the available amount of memory on our GPU (16GB). The number of features is set to the default output size of the ResNet50 backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 8\n",
    "batch_size = 128\n",
    "input_size = 128\n",
    "num_ftrs = 2048\n",
    "\n",
    "seed = 1\n",
    "max_epochs = 5\n",
    "\n",
    "# use cuda if possible\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have downloaded the dataset somewhere else or are using a different one. Set the path to the dataset accordingly. Additionally, make sure to set the path to the config file of the Detectron2 model you want to use. We will be using an RCNN with a feature pyramid network (FPN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/data/greiburg_groceries/freiburg_groceries_dataset/images\"\n",
    "cfg_path = \"/workspace/Base-RCNN-FPN.yaml\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the Detectron2 Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the Detectron2 ResNet50 backbone is a dictionary with the keys res1 through res5 (see the documentation). The keys correspond to the different stages of the ResNet. In this tutorial, we are only interested in the high-level abstractions from the last layer, res5. Therefore, we have to add an additional layer which picks the right output from the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectStage(torch.nn.Module):\n",
    "    \"\"\"Selects features from a given stage.\"\"\"\n",
    "\n",
    "    def __init__(self, stage: str = \"res5\"):\n",
    "        super().__init__()\n",
    "        self.stage = stage\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[self.stage]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s load the config file and make some adjustments to ensure smooth training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = config.get_cfg()\n",
    "cfg.merge_from_file(cfg_path)\n",
    "\n",
    "# use cuda if possible\n",
    "cfg.MODEL.DEVICE = device\n",
    "\n",
    "# randomly initialize network\n",
    "cfg.MODEL.WEIGHTS = \"\"\n",
    "\n",
    "# detectron2 uses BGR by default but pytorch/torchvision use RGB\n",
    "cfg.INPUT.FORMAT = \"RGB\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can build the Detectron2 model and extract the ResNet50 backbone as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "detmodel = modeling.build_model(cfg)\n",
    "\n",
    "simclr_backbone = torch.nn.Sequential(\n",
    "    detmodel.backbone.bottom_up,\n",
    "    SelectStage(\"res5\"),\n",
    "    # res5 has shape bsz x 2048 x 4 x 4\n",
    "    torch.nn.AdaptiveAvgPool2d(1),\n",
    ").to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let’s build SimCLR around the backbone as shown in the other tutorials. For this, we only require an additional projection head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_head = SimCLRProjectionHead(\n",
    "    input_dim=num_ftrs,\n",
    "    hidden_dim=num_ftrs,\n",
    "    output_dim=128,\n",
    ").to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup data augmentations and loaders"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining the augmentations which should be used for training. We use the same ones as in the SimCLR paper but change the input size and minimum scale of the random crop to adjust to our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = SimCLRTransform(input_size=input_size)\n",
    "\n",
    "dataset_train_simclr = LightlyDataset(input_dir=data_path, transform=transform)\n",
    "\n",
    "dataloader_train_simclr = torch.utils.data.DataLoader(\n",
    "    dataset_train_simclr,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-supervised pre-training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all we need to do is define a loss and optimizer and start training!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  0] Mean Loss = 5.64\n",
      "[Epoch  1] Mean Loss = 5.34\n",
      "[Epoch  2] Mean Loss = 5.25\n",
      "[Epoch  3] Mean Loss = 5.15\n",
      "[Epoch  4] Mean Loss = 5.13\n"
     ]
    }
   ],
   "source": [
    "criterion = NTXentLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(simclr_backbone.parameters()) + list(projection_head.parameters()),\n",
    "    lr=1e-4,\n",
    ")\n",
    "\n",
    "for e in range(max_epochs):\n",
    "    mean_loss = 0.0\n",
    "    for (x0, x1), _, _ in dataloader_train_simclr:\n",
    "        x0 = x0.to(device)\n",
    "        x1 = x1.to(device)\n",
    "\n",
    "        y0 = projection_head(simclr_backbone(x0).flatten(start_dim=1))\n",
    "        y1 = projection_head(simclr_backbone(x1).flatten(start_dim=1))\n",
    "\n",
    "        # backpropagation\n",
    "        loss = criterion(y0, y1)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # update average loss\n",
    "        mean_loss += loss.detach().cpu().item() / len(dataloader_train_simclr)\n",
    "\n",
    "    print(f\"[Epoch {e:2d}] Mean Loss = {mean_loss:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing the checkpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the pre-trained backbone from the Detectron2 model. The code below shows how to save it as a Detectron2 checkpoint called my_model.pth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first module from the backbone (i.e. the detectron2 ResNet)\n",
    "# backbone:\n",
    "#     L ResNet50\n",
    "#     L SelectStage\n",
    "#     L AdaptiveAvgPool2d\n",
    "detmodel.backbone.bottom_up = simclr_backbone[0]\n",
    "\n",
    "checkpointer = DetectionCheckpointer(detmodel, save_dir=\"/workspace/weights\")\n",
    "checkpointer.save(\"resnet50-detectron-backbone-pre-train-detectron2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning with Detectron2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The checkpoint from above can now be used by any Detectron2 script. For example, you can use the train_net.py script in the Detectron2 tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: can't open file '/workspace/notebooks/train_net.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# !python train_net.py --config-file ../configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_1x.yaml \\\n",
    "#     MODEL.WEIGHTS path/to/my_model.pth \\\n",
    "#     MODEL.PIXEL_MEAN 123.675,116.280,103.530 \\\n",
    "#     MODEL.PIXEL_STD 58.395,57.120,57.375 \\\n",
    "#     INPUT.FORMAT RGB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SimCLRTransform applies an ImageNet normalization of the input images by default. Therefore, we have to normalize the input images at training time, too. Since Detectron2 uses an input space in the range 0 - 255, we use the numbers above."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model was pre-trained with images in the RGB input format, it’s necessary to set the permute the order of the pixel mean, and pixel std as shown above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
